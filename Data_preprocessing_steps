Steps in Data Preprocessing
Step 1: Import the necessary libraries ------------------------------------------------------------------------------
# importing libraries
import pandas as pd
import scipy
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns
import matplotlib.pyplot as plt

Step 2: Load the dataset-------------------------------------------------------------------------------------------------
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/structural virality/dataset/nytpopular.csv")
print(df.head())

Check the data info:-
df.info()

like_label = list()
for like in df['like_count']:
    if like <= 285:
        like_label.append('0')
    else:
        like_label.append('1')

# Update this class label into the dataframe
df = pd.concat([df.reset_index(drop=True), pd.DataFrame(like_label, columns=['popularity'])], axis=1)
df.head(4)

df.info()

To check null value:-
df.isnull().sum()

Step 3: Statistical Analysis---------------------------------------------------------------------------------------------------
df.describe()

df.drop(labels=['url', 'date', 'bag_of_phrases'], axis = 1, inplace=True)
df.head(n=4)

Step 4: Check the outliers---------------------------------------------------------------------------------------------------
# Box Plots
fig, axs = plt.subplots(4,1,dpi=95, figsize=(7,17))
i = 0
for col in df.columns:
	axs[i].boxplot(df[col], vert=False)
	axs[i].set_ylabel(col)
	i+=1
plt.show()

# Drop the outliers
# Identify the quartiles
q1, q3 = np.percentile(df['retweet_count'], [25, 75])
# Calculate the interquartile range
iqr = q3 - q1
# Calculate the lower and upper bounds
lower_bound = q1 - (1.5 * iqr)
upper_bound = q3 + (1.5 * iqr)
# Drop the outliers
clean_data = df[(df['retweet_count'] >= lower_bound) 
				& (df['retweet_count'] <= upper_bound)]

# Identify the quartiles
q1, q3 = np.percentile(clean_data['reply_count'], [25, 75])
# Calculate the interquartile range
iqr = q3 - q1
# Calculate the lower and upper bounds
lower_bound = q1 - (1.5 * iqr)
upper_bound = q3 + (1.5 * iqr)
# Drop the outliers
clean_data = clean_data[(clean_data['reply_count'] >= lower_bound) 
						& (clean_data['reply_count'] <= upper_bound)]

# Identify the quartiles
q1, q3 = np.percentile(clean_data['like_count'], [25, 75])
# Calculate the interquartile range
iqr = q3 - q1
# Calculate the lower and upper bounds
lower_bound = q1 - (1.5 * iqr)
upper_bound = q3 + (1.5 * iqr)
# Drop the outliers
clean_data = clean_data[(clean_data['like_count'] >= lower_bound) 
						& (clean_data['like_count'] <= upper_bound)]

Step5: Correlation---------------------------------------------------------------------------------------------------
#correlation
corr = df.corr()
 
plt.figure(dpi=130)
sns.heatmap(df.corr(), annot=True, fmt= '.2f')
plt.show()

# We can also camapare by single columns in descending order
corr['like_count'].sort_values(ascending = False)

# Check Outcomes Proportionality
plt.pie(df.popularity.value_counts(), 
        labels= ['popular', 'unpopular'], 
        autopct='%.f', shadow=True)
plt.title('Popularity Proportionality')
plt.show()

Step 6: Separate independent features and Target Variables -----------------------------------------------------------------
# separate array into input and output components
X = df.drop(columns =['popularity'])
Y = df.popularity

Step 7: Normalization or Standardization ----------------------------------------------------------------------------------
# initialising the MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
 
# learning the statistical parameters for each of the data and transforming
rescaledX = scaler.fit_transform(X)
rescaledX[:5]

OR

# Standardization

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().fit(X)
rescaledX = scaler.transform(X)
rescaledX[:5]
